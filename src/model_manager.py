"""
ç»Ÿä¸€æ¨¡å‹ç®¡ç†å™¨
æ•´åˆVLMå’Œå†³ç­–æ¨¡å‹ä¸ºå•ä¸€Qwen2.5-VL-3Bæ¨¡å‹
"""
import logging
import torch
import time
import hashlib
import asyncio
from typing import List, Dict, Optional, Tuple
from collections import OrderedDict
from concurrent.futures import ThreadPoolExecutor

from modelscope import snapshot_download
from transformers import (
    Qwen2_5_VLForConditionalGeneration,
    Qwen2_5_VLProcessor,
    BitsAndBytesConfig
)
from PIL import Image
import base64
import io

from config.settings import settings


class LRUCache:
    """LRUç¼“å­˜å®ç°ï¼Œç”¨äºè·¯ç”±ç»“æœç¼“å­˜"""
    def __init__(self, max_size: int = 256):
        self.cache = OrderedDict()
        self.max_size = max_size
    
    def get(self, key: str) -> Optional[Tuple[str, List[Dict]]]:
        if key in self.cache:
            self.cache.move_to_end(key)
            return self.cache[key]
        return None
    
    def put(self, key: str, value: Tuple[str, List[Dict]]):
        if key in self.cache:
            self.cache.move_to_end(key)
        self.cache[key] = value
        if len(self.cache) > self.max_size:
            self.cache.popitem(last=False)
    
    def clear(self):
        self.cache.clear()


class UnifiedModelManager:
    """ç»Ÿä¸€æ¨¡å‹ç®¡ç†å™¨ - æ•´åˆVLMå’Œå†³ç­–åŠŸèƒ½"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # æ€§èƒ½ä¼˜åŒ–é…ç½®
        self.executor = ThreadPoolExecutor(max_workers=settings.MAX_WORKERS)
        self.enable_perf_logging = True
        
        # è·¯ç”±ç¼“å­˜
        self.route_cache = LRUCache(max_size=settings.ROUTE_CACHE_SIZE)
        self.full_labels = list(settings.MODEL_MAP.keys())
        
        # å†…å­˜ç®¡ç†
        self.inference_count = 0
        self.last_memory_cleanup = 0
        
        # æ¨¡å‹ç»„ä»¶
        self.model = None
        self.processor = None
        
        self.logger.info(f"ğŸ§  Initializing Unified Model Manager...")
        self.logger.info(f"ğŸ¯ Target Model: {settings.UNIFIED_MODEL_ID}")
        self.logger.info(f"ğŸ’¾ Device: {self.device}")
        
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=getattr(logging, settings.LOG_LEVEL),
            format=settings.LOG_FORMAT
        )
        
        # æŠ‘åˆ¶ç¬¬ä¸‰æ–¹åº“æ—¥å¿—
        logging.getLogger("litellm").setLevel(logging.WARNING)
        logging.getLogger("LiteLLM").setLevel(logging.WARNING)
        
    def load_model(self):
        """åŠ è½½ç»Ÿä¸€æ¨¡å‹"""
        try:
            self.logger.info("ğŸ§  Loading Unified Qwen2.5-VL-3B Model...")
            
            # ä¸‹è½½æ¨¡å‹
            model_dir = snapshot_download(settings.UNIFIED_MODEL_ID)
            
            # 4bité‡åŒ–é…ç½® - æ›´æ¿€è¿›çš„å†…å­˜ä¼˜åŒ–
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
                llm_int8_threshold=6.0,
                llm_int8_skip_modules=None,
                bnb_4bit_use_fp16=False  # ç¦ç”¨FP16ä»¥èŠ‚çœå†…å­˜
            )
            
            # åŠ è½½æ¨¡å‹ - ä½¿ç”¨æ›´æ¿€è¿›çš„å†…å­˜ä¼˜åŒ–
            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                model_dir,
                quantization_config=bnb_config,
                device_map="auto",
                max_memory={0: "6GB", "cpu": "16GB"},
                offload_folder=settings.OFFLOAD_FOLDER,
                offload_state_dict=settings.OFFLOAD_STATE_DICT,
                low_cpu_mem_usage=settings.LOW_CPU_MEM_USAGE,
                trust_remote_code=True,
                torch_dtype=torch.float16,
                use_safetensors=True
            )
            
            # åŠ è½½å¤„ç†å™¨
            self.processor = Qwen2_5_VLProcessor.from_pretrained(model_dir, trust_remote_code=True)
            
            # å°è¯•ä½¿ç”¨torch.compileåŠ é€Ÿ
            if settings.ENABLE_TORCH_COMPILE and hasattr(torch, 'compile') and self.device == "cuda":
                try:
                    self.logger.info("âš¡ Using torch.compile for optimization...")
                    self.model = torch.compile(self.model, mode="reduce-overhead")
                except Exception as e:
                    self.logger.warning(f"torch.compile not available: {e}")
            
            self.logger.info("âœ… Unified model loaded successfully")
            self._warmup_model()
            
        except Exception as e:
            self.logger.error(f"Failed to load model: {e}")
            raise e
    
    def _warmup_model(self):
        """æ¨¡å‹é¢„çƒ­"""
        self.logger.info("ğŸ”¥ Warming up model...")
        try:
            dummy_text = "Hello, this is a warmup test."
            self._get_router_scores(dummy_text)
            self.logger.info("âœ… Model warmup complete")
        except Exception as e:
            self.logger.warning(f"Warmup failed: {e}")
    
    def _decode_base64_image(self, base64_string: str) -> Optional[Image.Image]:
        """è§£ç base64å›¾åƒ"""
        try:
            image_data = base64.b64decode(base64_string)
            return Image.open(io.BytesIO(image_data)).convert('RGB')
        except Exception as e:
            self.logger.error(f"Failed to decode image: {e}")
            return None
    
    def _process_messages_for_vlm(self, messages: List[Dict]) -> Tuple[List[Dict], Optional[Image.Image]]:
        """å¤„ç†æ¶ˆæ¯ï¼Œæå–å›¾åƒå’Œæ–‡æœ¬"""
        processed_messages = []
        extracted_image = None
        
        for message in messages:
            new_message = message.copy()
            content = message.get("content", "")
            
            if isinstance(content, list):
                new_content = []
                for item in content:
                    if isinstance(item, dict):
                        if item.get("type") == "text":
                            new_content.append({"type": "text", "text": item.get("text", "")})
                        elif item.get("type") in ["image", "image_url"]:
                            # å¤„ç†å›¾åƒ
                            image_url = item.get("image_url", {})
                            if isinstance(image_url, dict):
                                image_url = image_url.get("url", "")
                            elif isinstance(image_url, str):
                                pass  # å·²ç»æ˜¯å­—ç¬¦ä¸²
                            else:
                                image_url = str(image_url)
                                
                            if image_url.startswith("data:image"):
                                # Base64ç¼–ç çš„å›¾åƒ
                                base64_string = image_url.split(",")[1]
                                extracted_image = self._decode_base64_image(base64_string)
                                if extracted_image:
                                    new_content.append({"type": "image", "image": extracted_image})
                            elif image_url.startswith("http"):
                                # URLå›¾åƒï¼ˆç®€åŒ–å¤„ç†ï¼‰
                                self.logger.warning("URL images not supported")
                            else:
                                # å‡è®¾æ˜¯base64å­—ç¬¦ä¸²
                                extracted_image = self._decode_base64_image(image_url)
                                if extracted_image:
                                    new_content.append({"type": "image", "image": extracted_image})
                        else:
                            new_content.append(item)
                    else:
                        # å¦‚æœitemä¸æ˜¯å­—å…¸ï¼Œç›´æ¥æ·»åŠ 
                        new_content.append(item)
                new_message["content"] = new_content
            else:
                new_message["content"] = content
                
            processed_messages.append(new_message)
        
        return processed_messages, extracted_image
    
    def _unified_vlm_inference(self, messages: List[Dict], prompt_text: str = None) -> str:
        """ä½¿ç”¨ç»Ÿä¸€æ¨¡å‹è¿›è¡ŒVLMæ¨ç†"""
        try:
            self.logger.info("ğŸ‘ï¸ Using Unified VLM for vision understanding...")
            
            # å¤„ç†æ¶ˆæ¯ï¼Œæå–å›¾åƒ
            processed_messages, extracted_image = self._process_messages_for_vlm(messages)
            
            if not extracted_image:
                self.logger.warning("No image found in messages")
                return ""
            
            # æ„å»ºè¾“å…¥
            if prompt_text is None:
                prompt_text = "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬æ–‡å­—ã€ç‰©ä½“ã€åœºæ™¯ç­‰æ‰€æœ‰å¯è§ä¿¡æ¯ã€‚"
            
            # å‡†å¤‡å¯¹è¯æ ¼å¼
            conversation = []
            for msg in processed_messages:
                role = msg["role"]
                content = msg["content"]
                
                if isinstance(content, list):
                    text_content = ""
                    has_image = False
                    for item in content:
                        if item.get("type") == "text":
                            text_content += item.get("text", "")
                        elif item.get("type") == "image":
                            has_image = True
                    if has_image:
                        conversation.append({"role": role, "content": [{"type": "image", "image": extracted_image}, {"type": "text", "text": text_content}]})
                    else:
                        conversation.append({"role": role, "content": text_content})
                else:
                    conversation.append({"role": role, "content": content})
            
            # æ·»åŠ ç”¨æˆ·é—®é¢˜
            conversation.append({"role": "user", "content": prompt_text})
            
            # å¤„ç†è¾“å…¥
            text = self.processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)
            inputs = self.processor(text=[text], images=[extracted_image], padding=True, return_tensors="pt")
            inputs = inputs.to(self.model.device)
            
            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                generated_ids = self.model.generate(**inputs, max_new_tokens=1024, temperature=0.1)
            
            # è§£ç å›ç­”
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            output_text = self.processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True)[0]
            
            self.logger.info("âœ… Unified VLM inference complete")
            return output_text
            
        except Exception as e:
            self.logger.error(f"Unified VLM Error: {e}")
            return ""
    
    def _quick_keyword_match(self, text: str) -> Optional[Tuple[str, float]]:
        """å¢å¼ºçš„å¿«é€Ÿå…³é”®è¯åŒ¹é… - æ”¯æŒæƒé‡ã€çŸ­è¯­åŒ¹é…å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥"""
        text_lower = text.lower()
        text_words = text_lower.split()
        text_length = len(text_lower)
        
        # åŠ æƒå…³é”®è¯é…ç½®
        weighted_keywords = {
            'code_technical': {
                'high_weight': ['def ', 'class ', 'function(', 'import ', 'from ', 'sql ', 'query(', 'debug'],
                'medium_weight': ['python', 'javascript', 'java', 'c++', 'typescript', 'go', 'rust', 'ä»£ç ', 'ç¼–ç¨‹', 'api', 'framework', 'library'],
                'low_weight': ['algorithm', 'data structure', 'module', 'package', 'variable', 'array', 'list', 'dictionary']
            },
            'code_architect': {
                'high_weight': ['architecture', 'design pattern', 'system design', 'microservice', 'æ¶æ„', 'è®¾è®¡æ¨¡å¼', 'scalability'],
                'medium_weight': ['api design', 'database design', 'performance optimization', 'concurrency', 'distributed system', 'restful', 'graphql'],
                'low_weight': ['pattern', 'principle', 'solid', 'dry', 'clean code', 'refactoring']
            },
            'logic_reasoning': {
                'high_weight': ['prove', 'theorem', 'calculate', 'solve(', 'equation', 'integral', 'å¾®åˆ†', 'ç§¯åˆ†', 'è¯æ˜', 'æ¨å¯¼'],
                'medium_weight': ['mathematical', 'proof', 'derivation', 'formula', 'probability', 'statistics', 'logic', 'algorithmic'],
                'low_weight': ['math', 'calculation', 'number', 'value', 'result']
            },
            'pro_advanced': {
                'high_weight': ['creative writing', 'story', 'poem', 'novel', 'åˆ›ä½œ', 'æ•…äº‹', 'è¯—æ­Œ', 'å°è¯´', 'essay'],
                'medium_weight': ['analysis', 'detailed explanation', 'comprehensive', 'in-depth', 'review', 'critique', 'interpretation'],
                'low_weight': ['write', 'describe', 'explain', 'discuss', 'analyze']
            },
            'flash_smart': {
                'high_weight': ['hello', 'hi ', 'hey', 'ä½ å¥½', 'è°¢è°¢', 'thanks', 'good morning', 'good evening'],
                'medium_weight': ['how are you', 'what is', 'tell me about', 'help me', 'can you'],
                'low_weight': ['question', 'answer', 'ask', 'say', 'tell']
            },
            'expert_xhigh': {
                'high_weight': ['research paper', 'academic', 'thesis', 'dissertation', 'ç ”ç©¶', 'å­¦æœ¯', 'è®ºæ–‡', 'æ–‡çŒ®ç»¼è¿°'],
                'medium_weight': ['methodology', 'hypothesis', 'empirical', 'theoretical framework', 'peer review', 'citation', 'journal'],
                'low_weight': ['study', 'analysis', 'investigation', 'experiment', 'data']
            }
        }
        
        # å¦å®šå…³é”®è¯ - è¿™äº›è¯å‡ºç°æ—¶åº”é™ä½å¯¹åº”ç±»åˆ«çš„åˆ†æ•°
        negative_keywords = {
            'code_technical': ['hello', 'how are you', 'what is your name', 'tell me a joke', 'story'],
            'pro_advanced': ['code', 'function', 'class', 'debug', 'calculate', 'prove'],
            'flash_smart': ['function', 'class ', 'import ', 'algorithm', 'architecture', 'theorem', 'research paper']
        }
        
        scores = {}
        
        for label, weight_groups in weighted_keywords.items():
            total_score = 0.0
            match_count = 0
            
            # æ£€æŸ¥é«˜æƒé‡å…³é”®è¯ (æƒé‡: 3.0)
            for kw in weight_groups['high_weight']:
                if kw in text_lower:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å®Œæ•´å•è¯åŒ¹é…ï¼ˆé¿å…éƒ¨åˆ†åŒ¹é…ï¼‰
                    if kw.strip() in text_words or ' ' in kw:
                        total_score += 3.0
                        match_count += 1
                    else:
                        total_score += 2.0
                        match_count += 1
            
            # æ£€æŸ¥ä¸­æƒé‡å…³é”®è¯ (æƒé‡: 2.0)
            for kw in weight_groups['medium_weight']:
                if kw in text_lower:
                    total_score += 2.0
                    match_count += 1
            
            # æ£€æŸ¥ä½æƒé‡å…³é”®è¯ (æƒé‡: 1.0)
            for kw in weight_groups['low_weight']:
                if kw in text_lower:
                    total_score += 1.0
                    match_count += 1
            
            # åº”ç”¨å¦å®šå…³é”®è¯æƒ©ç½š
            if label in negative_keywords:
                for neg_kw in negative_keywords[label]:
                    if neg_kw in text_lower:
                        total_score -= 2.0  # æƒ©ç½š
            
            # é•¿åº¦å¥–åŠ±ï¼šè¾ƒé•¿çš„æŸ¥è¯¢æ›´å¯èƒ½éœ€è¦å¤æ‚å¤„ç†
            if text_length > 100 and label != 'flash_smart':
                total_score *= 1.1
            
            # çŸ­æŸ¥è¯¢æƒ©ç½šï¼šéå¸¸çŸ­çš„æŸ¥è¯¢ä¸å¤ªå¯èƒ½æ˜¯å¤æ‚ä»»åŠ¡
            if text_length < 30 and label in ['code_architect', 'expert_xhigh']:
                total_score *= 0.5
            
            if match_count > 0 and total_score > 0:
                scores[label] = total_score
        
        if scores:
            best_label = max(scores, key=scores.get)
            # è°ƒæ•´é˜ˆå€¼ï¼šéœ€è¦è‡³å°‘3åˆ†æˆ–æœ‰2ä¸ªä»¥ä¸ŠåŒ¹é…
            if scores[best_label] >= 3.0:
                return best_label, scores[best_label]
        
        return None, 0.0
    
    def _normalize_scores(self, raw_scores: Dict[str, float]) -> Dict[str, float]:
        """å½’ä¸€åŒ–åˆ†æ•°"""
        scores = list(raw_scores.values())
        if not scores:
            return raw_scores
        
        min_score, max_score = min(scores), max(scores)
        
        if max_score == min_score:
            return {label: 5.0 for label in raw_scores.keys()}
        
        return {
            label: 1.0 + 9.0 * (score - min_score) / (max_score - min_score)
            for label, score in raw_scores.items()
        }
    
    def _improve_routing_decision(self, router_scores: Dict[str, float], text: str) -> Dict[str, float]:
        """å¢å¼ºçš„è·¯ç”±å†³ç­–é€»è¾‘ - å¤šç»´åº¦åˆ†æã€åŠ¨æ€æƒé‡è°ƒæ•´"""
        # é¦–å…ˆå½’ä¸€åŒ–åˆ†æ•°
        normalized_scores = self._normalize_scores(router_scores)
        
        # æ–‡æœ¬åˆ†æ
        text_lower = text.lower()
        text_length = len(text)
        word_count = len(text.split())
        sentence_count = text.count('.') + text.count('!') + text.count('?') + 1
        
        # è®¡ç®—æ–‡æœ¬ç‰¹å¾
        avg_word_length = sum(len(word) for word in text_lower.split()) / max(word_count, 1)
        complexity_score = (text_length / 100) * (avg_word_length / 5) * (word_count / 20)
        
        # æ‰©å±•çš„å…³é”®è¯æ£€æµ‹ - æ”¯æŒå¤šè¯­è¨€
        category_keywords = {
            'code_technical': {
                'programming': ['def ', 'class ', 'function(', 'import ', 'from ', 'lambda', 'async', 'await'],
                'languages': ['python', 'javascript', 'java', 'c++', 'typescript', 'go', 'rust', 'swift', 'kotlin'],
                'concepts': ['algorithm', 'data structure', 'recursion', 'iteration', 'string manipulation', 'regex', 'api', 'rest', 'graphql'],
                'chinese': ['ä»£ç ', 'ç¼–ç¨‹', 'å‡½æ•°', 'ç±»', 'è°ƒè¯•', 'ç®—æ³•', 'æ•°æ®ç»“æ„', 'æ¥å£'],
                'actions': ['debug', 'implement', 'refactor', 'optimize', 'write', 'create', 'build']
            },
            'code_architect': {
                'high_level': ['architecture', 'design pattern', 'system design', 'microservice', 'monolith', 'serverless'],
                'principles': ['scalability', 'maintainability', 'reliability', 'performance', 'security', 'coupling', 'cohesion'],
                'patterns': ['singleton', 'factory', 'observer', 'strategy', 'decorator', 'adapter', 'mvc', 'mvvm'],
                'chinese': ['æ¶æ„', 'è®¾è®¡æ¨¡å¼', 'å¯æ‰©å±•æ€§', 'å¾®æœåŠ¡', 'åˆ†å¸ƒå¼', 'é«˜å¹¶å‘', 'è§£è€¦'],
                'areas': ['database design', 'api design', 'system integration', 'cloud architecture', 'devops']
            },
            'logic_reasoning': {
                'math': ['prove', 'theorem', 'lemma', 'corollary', 'calculate', 'solve', 'equation', 'inequality', 'integral', 'derivative'],
                'logic': ['if and only if', 'therefore', 'hence', 'implies', 'contradiction', 'induction', 'deduction'],
                'chinese': ['è¯æ˜', 'å®šç†', 'æ¨å¯¼', 'å¾®åˆ†', 'ç§¯åˆ†', 'æ–¹ç¨‹', 'ä¸ç­‰å¼', 'å½’çº³'],
                'topics': ['probability', 'statistics', 'combinatorics', 'graph theory', 'number theory', 'geometry', 'calculus'],
                'actions': ['calculate', 'compute', 'determine', 'find', 'solve', 'derive']
            },
            'pro_advanced': {
                'creative': ['story', 'poem', 'novel', 'essay', 'article', 'blog post', 'creative writing', 'narrative'],
                'analysis': ['analyze', 'critique', 'review', 'evaluate', 'assess', 'interpret', 'examine'],
                'chinese': ['åˆ›ä½œ', 'æ•…äº‹', 'è¯—æ­Œ', 'å°è¯´', 'åˆ†æ', 'è¯„è®º', 'è§£è¯»', 'æ·±åº¦'],
                'requirements': ['detailed explanation', 'comprehensive', 'in-depth', 'thorough', 'extensive', 'nuanced'],
                'domains': ['literature', 'philosophy', 'psychology', 'sociology', 'cultural studies', 'history']
            },
            'expert_xhigh': {
                'academic': ['research', 'paper', 'journal', 'conference', 'publication', 'citation', 'bibliography'],
                'research': ['methodology', 'hypothesis', 'empirical', 'theoretical', 'qualitative', 'quantitative', 'peer review'],
                'chinese': ['ç ”ç©¶', 'å­¦æœ¯', 'è®ºæ–‡', 'æ–‡çŒ®', 'æ–¹æ³•è®º', 'å‡è®¾', 'å®è¯', 'ç†è®º'],
                'domains': ['machine learning', 'deep learning', 'artificial intelligence', 'data science', 'bioinformatics', 'economics'],
                'tasks': ['literature review', 'meta-analysis', 'systematic review', 'comparative study']
            },
            'flash_smart': {
                'greetings': ['hello', 'hi ', 'hey', 'good morning', 'good evening', 'good afternoon'],
                'chinese': ['ä½ å¥½', 'æ‚¨å¥½', 'æ—©ä¸Šå¥½', 'æ™šä¸Šå¥½', 'è°¢è°¢', 'æ„Ÿè°¢'],
                'casual': ['how are you', 'what\'s up', 'how\'s it going', 'thanks', 'thank you'],
                'simple': ['what is', 'tell me', 'help me', 'can you', 'please', 'question'],
                'social': ['joke', 'funny', 'interesting', 'tell me about', 'what do you think']
            }
        }
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å…³é”®è¯åŒ¹é…åˆ†æ•°
        category_match_scores = {}
        for category, keyword_groups in category_keywords.items():
            match_score = 0.0
            for group_name, keywords in keyword_groups.items():
                group_matches = sum(1 for kw in keywords if kw in text_lower)
                if group_matches > 0:
                    # æ ¹æ®ç»„çš„é‡è¦æ€§åŠ æƒ
                    weight = 1.5 if group_name in ['programming', 'languages', 'chinese', 'high_level', 'math', 'creative', 'academic', 'greetings'] else 1.0
                    match_score += group_matches * weight
            category_match_scores[category] = match_score
        
        # è°ƒæ•´åˆ†æ•°
        adjusted_scores = normalized_scores.copy()
        
        # 1. åŸºäºå¤æ‚åº¦è°ƒæ•´
        if complexity_score > 2.0:
            adjusted_scores['flash_smart'] *= 0.3
            adjusted_scores['pro_advanced'] *= 1.3
            adjusted_scores['expert_xhigh'] *= 1.2
        elif complexity_score > 1.0:
            adjusted_scores['flash_smart'] *= 0.5
            adjusted_scores['pro_advanced'] *= 1.1
        
        # 2. åŸºäºå…³é”®è¯åŒ¹é…è°ƒæ•´
        max_match_score = max(category_match_scores.values()) if category_match_scores else 0
        if max_match_score > 0:
            for category, match_score in category_match_scores.items():
                if match_score > 0:
                    # åŒ¹é…åˆ†æ•°è¶Šé«˜ï¼Œæå‡å¹…åº¦è¶Šå¤§
                    boost_factor = 1.0 + (match_score / max_match_score) * 0.5
                    adjusted_scores[category] *= boost_factor
                    
                    # åŒæ—¶æŠ‘åˆ¶å…¶ä»–ç±»åˆ«
                    if category != 'flash_smart':
                        adjusted_scores['flash_smart'] *= 0.8
        
        # 3. ç‰¹æ®Šè§„åˆ™è°ƒæ•´
        # ä»£ç ç›¸å…³ï¼šå¦‚æœæœ‰å¤šä¸ªä»£ç å…³é”®è¯ï¼Œå¼ºçƒˆå€¾å‘äºcodeç±»åˆ«
        if category_match_scores['code_technical'] >= 3 or category_match_scores['code_architect'] >= 2:
            adjusted_scores['code_technical'] *= 1.8
            adjusted_scores['code_architect'] *= 1.6
            adjusted_scores['flash_smart'] *= 0.2
            adjusted_scores['pro_advanced'] *= 0.6
        
        # æ•°å­¦/é€»è¾‘ç›¸å…³
        if category_match_scores['logic_reasoning'] >= 2:
            adjusted_scores['logic_reasoning'] *= 2.0
            adjusted_scores['flash_smart'] *= 0.3
            adjusted_scores['pro_advanced'] *= 0.7
        
        # å­¦æœ¯ç ”ç©¶ç›¸å…³
        if category_match_scores['expert_xhigh'] >= 2:
            adjusted_scores['expert_xhigh'] *= 2.2
            adjusted_scores['flash_smart'] *= 0.1
            adjusted_scores['pro_advanced'] *= 1.2
        
        # åˆ›æ„å†™ä½œç›¸å…³
        if category_match_scores['pro_advanced'] >= 2:
            adjusted_scores['pro_advanced'] *= 1.8
            adjusted_scores['flash_smart'] *= 0.5
        
        # 4. ä¸Šä¸‹æ–‡æ„ŸçŸ¥è°ƒæ•´
        # å¦‚æœæ–‡æœ¬å¾ˆçŸ­ä¸”æ²¡æœ‰å¤æ‚å…³é”®è¯ï¼Œå€¾å‘äºflash_smart
        if text_length < 50 and max_match_score < 2:
            adjusted_scores['flash_smart'] *= 2.0
            for category in ['code_technical', 'code_architect', 'logic_reasoning', 'expert_xhigh']:
                adjusted_scores[category] *= 0.3
        
        # å¦‚æœæ–‡æœ¬éå¸¸é•¿ï¼Œå€¾å‘äºé«˜çº§åˆ†æ
        if text_length > 500:
            adjusted_scores['pro_advanced'] *= 1.3
            adjusted_scores['expert_xhigh'] *= 1.2
            adjusted_scores['flash_smart'] *= 0.4
        
        # 5. é˜²æ­¢åˆ†æ•°è¿‡ä½
        min_score = 0.5
        for label in adjusted_scores:
            adjusted_scores[label] = max(min_score, adjusted_scores[label])
        
        # 6. æœ€ç»ˆå½’ä¸€åŒ–ï¼Œç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´å†…
        total = sum(adjusted_scores.values())
        if total > 0:
            target_total = len(adjusted_scores) * 5.0  # å¹³å‡åˆ†çº¦5
            scale_factor = target_total / total
            # é™åˆ¶ç¼©æ”¾å› å­ï¼Œé¿å…è¿‡åº¦æ”¾å¤§
            scale_factor = max(0.5, min(2.0, scale_factor))
            adjusted_scores = {k: v * scale_factor for k, v in adjusted_scores.items()}
        
        return adjusted_scores
    
    @torch.no_grad()
    def _get_router_scores(self, text: str) -> Dict[str, float]:
        """ä½¿ç”¨ç»Ÿä¸€æ¨¡å‹è¿›è¡Œè·¯ç”±å†³ç­–"""
        start_time = time.time()
        try:
            context_segment = text[:800]
            
            prompt = (
                "Rate the user input for EACH category below. You MUST rate ALL 6 categories.\n"
                "Score: 1 = Not relevant, 10 = Perfect match\n\n"
                "Categories:\n"
                "1. flash_smart: General chat, greetings, simple questions, daily conversation\n"
                "2. pro_advanced: Complex analysis, creative writing, nuanced language understanding, detailed explanations\n"
                "3. code_technical: Programming, debugging, SQL queries, writing code in Python/C++/Java, technical scripts\n"
                "4. code_architect: System design, software architecture, explaining technical concepts, architectural patterns\n"
                "5. logic_reasoning: Math proofs, physics problems, logic puzzles, step-by-step reasoning, calculus, theorems\n"
                "6. expert_xhigh: Professional research, academic papers, high-context analysis, specialized knowledge\n\n"
                f"User Input: \"{context_segment}\"\n\n"
                "Output ALL 6 ratings in format: label:X (one per line, where X is a number from 1 to 10)."
            )
            
            messages = [
                {"role": "system", "content": "You are a precise classifier. Rate each category from 1 to 10 based on relevance."},
                {"role": "user", "content": prompt}
            ]
            
            # ä½¿ç”¨ç»Ÿä¸€æ¨¡å‹è¿›è¡Œè·¯ç”±å†³ç­–
            text_input = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            model_inputs = self.processor(text=[text_input], return_tensors="pt").to(self.model.device)
            
            # ç”Ÿæˆè·¯ç”±å†³ç­–
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=settings.MAX_NEW_TOKENS,
                temperature=0.1,
                do_sample=False,
                num_beams=1,
                pad_token_id=self.processor.tokenizer.eos_token_id,
                use_cache=settings.ENABLE_KV_CACHE
            )
            
            generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]
            response = self.processor.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            # è§£æè¯„åˆ†
            scores = {}
            for line in response.strip().split('\n'):
                line = line.strip()
                if ':' not in line:
                    continue
                
                for separator in [':', '=', ' ']:
                    if separator in line:
                        parts = line.split(separator, 1)
                        if len(parts) == 2:
                            potential_label = parts[0].strip().lower()
                            potential_score = parts[1].strip()
                            
                            for label in self.full_labels:
                                if label.lower() in potential_label or potential_label in label.lower():
                                    score_str = ""
                                    for char in potential_score:
                                        if char.isdigit() or char == '.':
                                            score_str += char
                                        elif char in [' ', '\t'] and score_str:
                                            break
                                        elif char not in [' ', '\t'] and not (char.isdigit() or char == '.'):
                                            if score_str:
                                                break
                                    
                                    if score_str:
                                        try:
                                            score = float(score_str)
                                            if 0 <= score <= 10:
                                                scores[label] = score
                                                break
                                        except ValueError:
                                            continue
            
            for label in self.full_labels:
                if label not in scores:
                    scores[label] = 1.0
            
            if self.enable_perf_logging:
                self.logger.info(f"âš¡ Router: {(time.time() - start_time)*1000:.1f}ms")
            
            return scores
            
        except Exception as e:
            self.logger.error(f"Router scoring error: {e}")
            return {label: 1.0 for label in self.full_labels}
    
    def _get_text_hash(self, text: str) -> str:
        """ç”Ÿæˆæ–‡æœ¬hashç”¨äºç¼“å­˜"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()
    
    def _cleanup_memory(self):
        """æ¸…ç†GPUå†…å­˜"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            if settings.DEBUG:
                self.logger.info(f"ğŸ§¹ Memory cleaned. GPU memory: {torch.cuda.memory_allocated()/1024**3:.1f}GB")
    
    def _check_memory_cleanup(self):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†å†…å­˜"""
        self.inference_count += 1
        if (self.inference_count - self.last_memory_cleanup) >= settings.MEMORY_CLEANUP_INTERVAL:
            self._cleanup_memory()
            self.last_memory_cleanup = self.inference_count
    
    def _get_fused_decision(self, messages: List[Dict]) -> Tuple[str, List[Dict]]:
        """èåˆå†³ç­–é€»è¾‘"""
        decision_start = time.time()
        target_text = ""
        modified_messages = messages
        
        # æ£€æŸ¥å†…å­˜æ¸…ç†
        self._check_memory_cleanup()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å›¾åƒ
        has_image = any(
            isinstance(m.get("content"), list) and any(item.get("type") in ["image", "image_url"] for item in m["content"])
            for m in messages[-2:]
        )
        
        if has_image:
            self.logger.info("ğŸ“¸ Image detected. Starting Unified VLM processing...")
            try:
                extracted_text = self._unified_vlm_inference(messages)
                target_text = extracted_text
                modified_messages = []
                for m in messages:
                    new_m = m.copy()
                    if isinstance(new_m.get("content"), list):
                        new_m["content"] = f"ã€System Note: Image Content (OCR):ã€‘\n{extracted_text}"
                    modified_messages.append(new_m)
            except Exception as e:
                self.logger.error(f"VLM processing failed: {e}")
                # å¦‚æœVLMå¤±è´¥ï¼Œå›é€€åˆ°æ–‡æœ¬è·¯ç”±
                has_image = False
        
        if not has_image:
            last_msg = messages[-1]
            if isinstance(last_msg["content"], str):
                target_text = last_msg["content"]
            elif isinstance(last_msg["content"], list):
                for item in last_msg["content"]:
                    if item.get("type") == "text":
                        target_text += item.get("text", "")
        
        # æ£€æŸ¥ç¼“å­˜
        if not has_image and target_text:
            text_hash = self._get_text_hash(target_text)
            cached_result = self.route_cache.get(text_hash)
            if cached_result:
                self.logger.info(f"âš¡ Cache hit! Route: {cached_result[0]} ({((time.time() - decision_start)*1000):.1f}ms)")
                return cached_result
        
        # å¿«é€Ÿè·¯å¾„
        if target_text and len(target_text) < 500:
            quick_label, quick_score = self._quick_keyword_match(target_text)
            if quick_label:
                self.logger.info(f"âš¡ Quick path: {quick_label} (score: {quick_score:.1f}) ({((time.time() - decision_start)*1000):.1f}ms)")
                result = (quick_label, modified_messages)
                if not has_image and target_text:
                    text_hash = self._get_text_hash(target_text)
                    self.route_cache.put(text_hash, result)
                return result
        
        # ä½¿ç”¨ç»Ÿä¸€æ¨¡å‹è¿›è¡Œè·¯ç”±å†³ç­–
        try:
            router_scores = self._get_router_scores(target_text)
            
            # æ”¹è¿›çš„è·¯ç”±å†³ç­–é€»è¾‘
            final_scores = self._improve_routing_decision(router_scores, target_text)
            best_label = max(final_scores, key=final_scores.get)
            
            # Debugæ¨¡å¼ï¼šæ˜¾ç¤ºè¯¦ç»†è¯„åˆ†
            if settings.DEBUG:
                self.logger.info(f"ğŸ” Debug - Raw scores: {router_scores}")
                self.logger.info(f"ğŸ” Debug - Final scores: {final_scores}")
                self.logger.info(f"ğŸ” Debug - Best label: {best_label}")
            
            # ç¼“å­˜ç»“æœ
            if not has_image and target_text:
                result = (best_label, modified_messages)
                text_hash = self._get_text_hash(target_text)
                self.route_cache.put(text_hash, result)
            
            # æ—¥å¿—è¾“å‡º
            if self.enable_perf_logging:
                self.logger.info(f"ğŸ¯ Route: {best_label} ({((time.time() - decision_start)*1000):.1f}ms)")
            
            return best_label, modified_messages
            
        except Exception as e:
            self.logger.error(f"Routing decision failed: {e}")
            # å›é€€åˆ°é»˜è®¤è·¯ç”±
            return "flash_smart", modified_messages
    
    async def route(self, messages: List[Dict]) -> Tuple[str, List[Dict]]:
        """å¼‚æ­¥è·¯ç”±æ¥å£"""
        return await asyncio.get_event_loop().run_in_executor(self.executor, self._get_fused_decision, messages)
    
    def inject_assistant_prompt(self, messages: List[Dict]) -> List[Dict]:
        """æ³¨å…¥åŠ©æ‰‹æç¤º"""
        new_msgs = [m.copy() for m in messages]
        injection = {
            "role": "assistant",
            "content": "I will provide a professional solution. For code, I will optimize it. For math, I use LaTeX.\n"
        }
        new_msgs.append(injection)
        return new_msgs