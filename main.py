"""
ç»Ÿä¸€NekoBrain APIæœåŠ¡
æ•´åˆVLMå’Œå†³ç­–æ¨¡å‹ä¸ºå•ä¸€æœåŠ¡
"""
import os
import logging
import json
import traceback
import asyncio
import time
from typing import List, Dict, Optional
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import litellm

from src.model_manager import UnifiedModelManager
from config.settings import settings


# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=getattr(logging, settings.LOG_LEVEL),
    format=settings.LOG_FORMAT,
    handlers=[
        logging.FileHandler(settings.LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(settings.APP_NAME)

# æŠ‘åˆ¶ç¬¬ä¸‰æ–¹åº“æ—¥å¿—
logging.getLogger("litellm").setLevel(logging.WARNING)
logging.getLogger("LiteLLM").setLevel(logging.WARNING)
litellm.suppress_debug_info = True


# å…¨å±€æ¨¡å‹ç®¡ç†å™¨å®ä¾‹
brain: UnifiedModelManager = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    global brain
    
    logger.info("ğŸš€ Starting Unified NekoBrain Service...")
    
    try:
        # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        brain = UnifiedModelManager()
        brain._setup_logging()
        brain.load_model()
        
        logger.info("âœ… Service initialized successfully")
        logger.info(f"ğŸ¯ Model: {settings.UNIFIED_MODEL_ID}")
        logger.info(f"ğŸ’¾ Device: {brain.device}")
        logger.info(f"ğŸ”§ Workers: {settings.MAX_WORKERS}")
        
    except Exception as e:
        logger.error(f"âŒ Failed to initialize service: {e}")
        logger.error(traceback.format_exc())
        raise e
    
    yield
    
    # æ¸…ç†èµ„æº
    logger.info("ğŸ›‘ Shutting down service...")
    if brain:
        brain.route_cache.clear()
    logger.info("âœ… Service shutdown complete")


# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title=settings.APP_NAME,
    version=settings.APP_VERSION,
    description="ç»Ÿä¸€VLMå’Œå†³ç­–æ¨¡å‹æœåŠ¡ - é’ˆå¯¹RTX 2060 12GBä¼˜åŒ–",
    lifespan=lifespan
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.CORS_ALLOW_ORIGINS,
    allow_credentials=True,
    allow_methods=settings.CORS_ALLOW_METHODS,
    allow_headers=settings.CORS_ALLOW_HEADERS,
)


# è¯·æ±‚æ¨¡å‹
class ChatRequest(BaseModel):
    messages: List[Dict]
    model: str
    stream: Optional[bool] = True


# å“åº”æ¨¡å‹
class HealthResponse(BaseModel):
    status: str
    model: str
    device: str
    version: str


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    if brain is None:
        raise HTTPException(status_code=503, detail="Service not initialized")
    
    return HealthResponse(
        status="healthy",
        model=settings.UNIFIED_MODEL_ID,
        device=brain.device,
        version=settings.APP_VERSION
    )


@app.get("/models")
async def list_models():
    """åˆ—å‡ºå¯ç”¨çš„è·¯ç”±æ¨¡å‹"""
    return {
        "unified_model": settings.UNIFIED_MODEL_ID,
        "target_models": settings.MODEL_MAP,
        "routing_categories": settings.ROUTING_CATEGORIES
    }


@app.post("/v1/chat/completions")
async def chat_completions(req: ChatRequest):
    """ç»Ÿä¸€çš„èŠå¤©å®Œæˆæ¥å£"""
    try:
        if brain is None:
            raise HTTPException(status_code=503, detail="Service not initialized")
        
        # ç»Ÿä¸€å¤„ç†auto_routeræ¨¡å‹åç§°
        if req.model and req.model.startswith("auto"):
            req.model = "auto_router"
        
        logger.info(f"ğŸ“¨ Received request: {req.model}")
        
        # è®°å½•å†³ç­–å¼€å§‹æ—¶é—´
        decision_start = time.time()
        
        # ä½¿ç”¨ç»Ÿä¸€æ¨¡å‹è¿›è¡Œè·¯ç”±å†³ç­–
        label, processed_msgs = await brain.route(req.messages)
        
        # è·å–ç›®æ ‡æ¨¡å‹
        target_model = settings.MODEL_MAP.get(label, "gemini-3-flash-preview")
        
        # ä¸ºä»£ç å’Œé€»è¾‘ä»»åŠ¡æ³¨å…¥åŠ©æ‰‹æç¤º
        if "code" in label or "logic" in label:
            processed_msgs = brain.inject_assistant_prompt(processed_msgs)
        
        logger.info(f"ğŸ¯ Routing to: {target_model} (category: {label})")
        
        # è°ƒç”¨èšåˆAPI
        resp = await litellm.acompletion(
            model=f"openai/{target_model}",
            messages=processed_msgs,
            stream=req.stream,
            api_base=settings.AGGREGATOR_BASE_URL,
            api_key=settings.AGGREGATOR_API_KEY
        )
        
        # å¤„ç†æµå¼å“åº”
        if req.stream:
            async def generate_stream():
                # å‘é€è·¯ç”±ä¿¡æ¯å‰ç¼€ - æ›´ç¾è§‚çš„æ˜¾ç¤º
                category_names = {
                    'flash_smart': 'ğŸ’¬ é€šç”¨å¯¹è¯',
                    'pro_advanced': 'ğŸ“ é«˜çº§åˆ†æ',
                    'code_technical': 'ğŸ’» æŠ€æœ¯ç¼–ç¨‹',
                    'code_architect': 'ğŸ—ï¸ æ¶æ„è®¾è®¡',
                    'logic_reasoning': 'ğŸ§® é€»è¾‘æ¨ç†',
                    'expert_xhigh': 'ğŸ“ ä¸“ä¸šç ”ç©¶'
                }
                
                category_name = category_names.get(label, label)
                model_names = {
                    'gemini-3-flash-preview': 'Gemini 3 Flash',
                    'gemini-3-pro-preview': 'Gemini 3 Pro',
                    'gpt-5-codex-high': 'GPT-5 Codex',
                    'claude-4-opus': 'Claude 4 Opus',
                    'gemini-3-pro-deepthink': 'Gemini 3 DeepThink',
                    'gpt-5.2-xhigh': 'GPT-5.2 XHigh'
                }
                
                model_name = model_names.get(target_model, target_model)
                
                prefix = f"> ğŸ§  **Unified NekoBrain v2.0**\n> ğŸ¯ æ™ºèƒ½è·¯ç”±: {category_name}\n> ğŸ¤– ç›®æ ‡æ¨¡å‹: {model_name}\n> âš¡ æ¨ç†æ—¶é—´: ~{((time.time() - decision_start)*1000):.0f}ms\n\n"
                yield f"data: {json.dumps({'choices': [{'delta': {'content': prefix}, 'index': 0}], 'model': target_model})}\n\n"
                
                # æµå¼å‘é€å“åº”
                async for chunk in resp:
                    yield f"data: {chunk.model_dump_json()}\n\n"
                
                # ç»“æŸæ ‡è®°
                yield "data: [DONE]\n\n"
            
            return StreamingResponse(generate_stream(), media_type="text/event-stream")
        
        # éæµå¼å“åº”
        return resp
        
    except Exception as e:
        logger.error(f"âŒ Request failed: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/stats")
async def get_stats():
    """è·å–æœåŠ¡ç»Ÿè®¡ä¿¡æ¯"""
    if brain is None:
        raise HTTPException(status_code=503, detail="Service not initialized")
    
    return {
        "cache_size": len(brain.route_cache.cache),
        "cache_max_size": brain.route_cache.max_size,
        "device": brain.device,
        "model": settings.UNIFIED_MODEL_ID,
        "performance_logging": brain.enable_perf_logging
    }


@app.post("/clear_cache")
async def clear_cache():
    """æ¸…ç©ºè·¯ç”±ç¼“å­˜"""
    if brain is None:
        raise HTTPException(status_code=503, detail="Service not initialized")
    
    brain.route_cache.clear()
    logger.info("ğŸ§¹ Route cache cleared")
    
    return {"status": "cache_cleared", "message": "è·¯ç”±ç¼“å­˜å·²æ¸…ç©º"}


@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "service": settings.APP_NAME,
        "version": settings.APP_VERSION,
        "model": settings.UNIFIED_MODEL_ID,
        "description": "ç»Ÿä¸€VLMå’Œå†³ç­–æ¨¡å‹æœåŠ¡",
        "endpoints": {
            "health": "/health",
            "models": "/models",
            "chat": "/v1/chat/completions",
            "stats": "/stats",
            "clear_cache": "/clear_cache"
        }
    }


if __name__ == "__main__":
    import uvicorn
    
    logger.info(f"ğŸš€ Starting {settings.APP_NAME} v{settings.APP_VERSION}")
    logger.info(f"ğŸŒ Server: http://{settings.HOST}:{settings.PORT}")
    logger.info(f"ğŸ¯ Model: {settings.UNIFIED_MODEL_ID}")
    
    uvicorn.run(
        "main:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=False,  # å…³é—­reloadæ¨¡å¼ä»¥é¿å…æ—¥å¿—å¹²æ‰°
        log_level=settings.LOG_LEVEL.lower()
    )